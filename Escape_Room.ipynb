{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mentel1/Escape-Room-RL/blob/main/Escape_Room.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NY2lnSuuUF-u",
        "nbgrader": {
          "checksum": "e95b77b39709873d39c9fa46afbf4700",
          "grade": false,
          "grade_id": "cell-82b38787563a7073",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## Packages.\n",
        "\n",
        "We import the following libraries that are required for this assignment. We shall be using the following libraries:\n",
        "1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
        "2. numpy: the fundamental package for scientific computing with Python.\n",
        "3. matplotlib: the library for plotting graphs in Python.\n",
        "4. RL-Glue: the library for reinforcement learning experiments.\n",
        "5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n",
        "6. Manager: the file allowing for visualization and testing.\n",
        "7. itertools.product: the function that can be used easily to compute permutations.\n",
        "8. tqdm.tqdm: Provides progress bars for visualizing the status of loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgiDMXGdVqSd",
        "outputId": "b8ef0ebc-8986-48ca-c6b4-2808f947290a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jdc in c:\\python27\\lib\\site-packages (0.0.9)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\n",
            "WARNING: You are using pip version 20.2.3; however, version 20.3.4 is available.\n",
            "You should consider upgrading via the 'c:\\python27\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\n",
            "ERROR: Could not find a version that satisfies the requirement rl_glue (from versions: none)\n",
            "ERROR: No matching distribution found for rl_glue\n",
            "WARNING: You are using pip version 20.2.3; however, version 20.3.4 is available.\n",
            "You should consider upgrading via the 'c:\\python27\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "! pip install jdc\n",
        "! pip install rl_glue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "r3l93PPIUF-v",
        "nbgrader": {
          "checksum": "386dc36257995b1270913ddfcdc1a838",
          "grade": false,
          "grade_id": "cell-917f710997077ab6",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import jdc\n",
        "import numpy as np\n",
        "from rl_glue import RLGlue\n",
        "import gym\n",
        "from Agent import BaseAgent \n",
        "from Environment import BaseEnvironment  \n",
        "from manager import Manager\n",
        "from itertools import product\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tiPfkIiuUF-w",
        "nbgrader": {
          "checksum": "83300358f840fb91d5c9ef9a3e77e09c",
          "grade": false,
          "grade_id": "cell-bb3b3151b3f4f759",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "# Section 1. Environment\n",
        "\n",
        "In the first part of this assignment, you will get to see how the Cliff Walking environment is implemented. You will also get to implement parts of it to aid your understanding of the environment and more generally how MDPs are specified. In particular, you will implement the logic for:\n",
        " 1. Converting 2-dimensional coordinates to a single index for the state,\n",
        " 2. One of the actions (action up),\n",
        " 3. Reward and termination.\n",
        " \n",
        "Given below is an annotated diagram of the environment with more details that may help in completing the tasks of this part of the assignment. Note that we will be creating a more general environment where the height and width positions can be variable but the start, goal and cliff grid cells have the same relative positions (bottom left, bottom right and the cells between the start and goal grid cells respectively).\n",
        "\n",
        "\n",
        "\n",
        "Once you have gone through the code and begun implementing solutions, it may be a good idea to come back here and see if you can convince yourself that the diagram above is an accurate representation of the code given and the code you have written."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NmS9lROIUF-x",
        "nbgrader": {
          "checksum": "54342eceb70d3ded536ef90b397df6a9",
          "grade": false,
          "grade_id": "cell-3b0342944fae98dd",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell!\n",
        "\n",
        "# Create empty EscapeRoomEnvironment class.\n",
        "# These methods will be filled in later cells.\n",
        "class EscapeRoomEnvironment(BaseEnvironment):\n",
        "    def env_init(self, agent_info={}):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def env_start(self, state):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def env_render(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def env_step(self, reward, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def env_end(self, reward):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def env_cleanup(self, reward):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    # helper method\n",
        "    def state(self, loc):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "aQcr2ce8UF-y",
        "nbgrader": {
          "checksum": "4583069bbb85ef2faa1824bf57693198",
          "grade": false,
          "grade_id": "cell-ace4da9bae087ba3",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## env_init()\n",
        "\n",
        "The first function we add to the environment is the initialization function which is called once when an environment object is created. In this function, the grid dimensions and special locations (start and goal locations and the cliff locations) are stored for easy use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oEEchbB9UF-z",
        "nbgrader": {
          "checksum": "386dd420b57b9661e21454eb72f9783e",
          "grade": false,
          "grade_id": "cell-fa1aefc241323c3d",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "%%add_to EscapeRoomEnvironment\n",
        "\n",
        "# Do not modify this cell!\n",
        "\n",
        "# Work Required: No.\n",
        "def env_init(self, env_info={}):\n",
        "        \"\"\"Setup for the environment called when the experiment first starts.\n",
        "        Note:\n",
        "            Initialize a tuple with the reward, first state, boolean\n",
        "            indicating if it's terminal.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Note, we can setup the following variables later, in env_start() as it is equivalent. \n",
        "        # Code is left here to adhere to the note above, but these variables are initialized once more\n",
        "        # in env_start() [See the env_start() function below.]\n",
        "        \n",
        "        reward = None\n",
        "        state = None # See Aside\n",
        "        termination = None\n",
        "        self.reward_state_term = (reward, state, termination)\n",
        "        \n",
        "        # AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably \n",
        "        # used with the term \"state\" for our purposes and for this assignment in particular. \n",
        "        # A difference arises in the use of the terms when we have what is called Partial Observability where \n",
        "        # the environment may return states that may not fully represent all the information needed to \n",
        "        # predict values or make decisions (i.e., the environment is non-Markovian.)\n",
        "        \n",
        "        self.grid_h = env_info.get(\"grid_height\", 5) \n",
        "        self.grid_w = env_info.get(\"grid_width\", 5)\n",
        "        self.grid_shape = (self.grid_h, self.grid_w)\n",
        "        \n",
        " \n",
        "        self.start_loc = (self.grid_h-1, self.grid_w//2)\n",
        "        # Goal location is the bottom-right corner. (max x, max y).\n",
        "        self.goal_loc = (0,self.grid_w//2)\n",
        "        # The door is in the middle of the top line of the room\n",
        "        self.obstacle_loc = (self.goal_loc[0]+1,self.goal_loc[1])\n",
        "        # There is an obstacle in front of the door\n",
        "\n",
        "        # map bounds\n",
        "        self.UP_map_bound = [(-1, y) for y in range(-1,self.grid_w+1)]\n",
        "        self.DOWN_map_bound = [(self.grid_h, y) for y in range(-1,self.grid_w+1)]\n",
        "        self.RIGHT_map_bound = [(x, -1) for x in range(-1,self.grid_h+1)]\n",
        "        self.LEFT_map_bound = [(x, self.grid_w) for x in range(-1,self.grid_h+1)]\n",
        "        self.forbidden_locs = self.UP_map_bound + self.DOWN_map_bound + self.RIGHT_map_bound + self.LEFT_map_bound + [self.obstacle_loc]\n",
        "        self.forbidden_locs = list(set(self.forbidden_locs))\n",
        "\n",
        "        self.key_loc = (self.grid_h-1,self.grid_w-1)\n",
        "        assert self.key_loc not in self.forbidden_locs, \"key location init is forbidden, try another location\"\n",
        "        assert self.start_loc not in self.forbidden_locs, \"start location init is forbidden, try another location\" \n",
        "        assert self.goal_loc not in self.forbidden_locs, \"goal location init is forbidden, try another location\" \n",
        "        #The key is in the bottom right corner\n",
        "        self.got_key = False\n",
        "        #The player does not have the key in the beginning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Evh6TfzzUF-1",
        "nbgrader": {
          "checksum": "b388d1d1cccc77907f4137339943df39",
          "grade": false,
          "grade_id": "cell-dd93e8a1b24bc4cf",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## env_start()\n",
        "\n",
        "In env_start(), we initialize the agent location to be the start location and return the state corresponding to it as the first state for the agent to act upon. Additionally, we also set the reward and termination terms to be 0 and False respectively as they are consistent with the notion that there is no reward nor termination before the first action is even taken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "J5cUtVdFUF-1",
        "nbgrader": {
          "checksum": "58fed26db20dabd6fa4570b1f399a148",
          "grade": false,
          "grade_id": "cell-c71fa14686edfa0b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "%%add_to EscapeRoomEnvironment\n",
        "\n",
        "# Do not modify this cell!\n",
        "\n",
        "# Work Required: No.\n",
        "def env_start(self):\n",
        "    \"\"\"The first method called when the episode starts, called before the\n",
        "    agent starts.\n",
        "\n",
        "    Returns:\n",
        "        The first state from the environment.\n",
        "    \"\"\"\n",
        "    reward = 0\n",
        "    # agent_loc will hold the current location of the agent\n",
        "    self.agent_loc = self.start_loc\n",
        "    # state is the one dimensional state representation of the agent location.\n",
        "    state = (*self.agent_loc,self.got_key)\n",
        "    termination = False\n",
        "    self.reward_state_term = (reward, state, termination)\n",
        "\n",
        "    return self.reward_state_term[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## env_render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%add_to EscapeRoomEnvironment\n",
        "\n",
        "def env_render(self):\n",
        "    \"\"\"render the current state to terminal\n",
        "    0 : background (' ')\n",
        "    1 : player ('P')\n",
        "    2 : door ('D')\n",
        "    3 : key ('K')\n",
        "    4 : left/right wall ('|')\n",
        "    5 : top/bottom wall ('-')\n",
        "    6 : obstacle ('X')\n",
        "    \"\"\"\n",
        "    lut = {0:' ', \n",
        "           1:gym.utils.colorize('P',\"blue\"),\n",
        "           2:gym.utils.colorize('D',\"green\"),\n",
        "           3:gym.utils.colorize('K',\"yellow\"),\n",
        "           4:'|',\n",
        "           5:'-',\n",
        "           6:gym.utils.colorize('X',\"red\"),\n",
        "           }\n",
        "           \n",
        "    r = np.zeros(self.grid_shape, dtype='int8')\n",
        "\n",
        "    r[self.goal_loc] = 2 # door\n",
        "    if not self.got_key : \n",
        "      r[self.key_loc] = 3 # key\n",
        "    r[self.obstacle_loc] = 6\n",
        "\n",
        "\n",
        "    agent_state = self.reward_state_term[1]\n",
        "\n",
        "    if agent_state is not None :\n",
        "      agent_loc = agent_state[:2]\n",
        "      r[agent_loc] = 1\n",
        "\n",
        "    r = np.pad(r, 1, mode='constant',constant_values=4)\n",
        "    r[0][:] = 5\n",
        "    r[-1][:] = 5\n",
        "    r_str = \"\"\n",
        "    for i in range(r.shape[0]):\n",
        "      for j in range(r.shape[1]):\n",
        "        r_str += lut[r[i,j]]\n",
        "      r_str += '\\n'\n",
        "    # r_str += '('+self.action_space_lut[self._game.player_last_action] + ')\\n'\n",
        "    return r_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------\n",
            "|   \u001b[32mD\u001b[0m  |\n",
            "|   \u001b[31mX\u001b[0m  |\n",
            "|      |\n",
            "|      |\n",
            "|      |\n",
            "|      |\n",
            "|      |\n",
            "|   \u001b[34mP\u001b[0m \u001b[33mK\u001b[0m|\n",
            "--------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "126"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = EscapeRoomEnvironment()\n",
        "init_params = {\n",
        "    \"grid_width\": 6,\n",
        "    \"grid_height\": 8,\n",
        "}\n",
        "\n",
        "env.env_init(env_info = init_params)\n",
        "env.env_start()\n",
        "\n",
        "# Render the game\n",
        "os.system(\"clear\")\n",
        "sys.stdout.write(env.env_render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fCsMkayBUF-1",
        "nbgrader": {
          "checksum": "1c21e0e861d5f3220fff345e3d5e07b0",
          "grade": false,
          "grade_id": "cell-bf5f7e78e0019780",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## *Implement* env_step()\n",
        "\n",
        "Once an action is taken by the agent, the environment must provide a new state, reward and termination signal. \n",
        "\n",
        "In the Cliff Walking environment, agents move around using a 4-cell neighborhood called the Von Neumann neighborhood (https://en.wikipedia.org/wiki/Von_Neumann_neighborhood). Thus, the agent has 4 available actions at each state. Three of the actions have been implemented for you and your first task is to implement the logic for the fourth action (Action UP).\n",
        "\n",
        "Your second task for this function is to implement the reward logic. Look over the environment description given earlier in this notebook if you need a refresher for how the reward signal is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "ZCJufdYeUF-2",
        "nbgrader": {
          "checksum": "e35e399a59401a069393c4fc781bad8c",
          "grade": false,
          "grade_id": "cell-45dfc79809d59695",
          "locked": false,
          "schema_version": 1,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "%%add_to CliffWalkEnvironment\n",
        "\n",
        "# Work Required: Yes. Fill in the code for action UP and implement the logic for reward and termination.\n",
        "# Lines: ~7.\n",
        "def env_step(self, action):\n",
        "    \"\"\"A step taken by the environment.\n",
        "\n",
        "    Args:\n",
        "        action: The action taken by the agent\n",
        "\n",
        "    Returns:\n",
        "        (float, state, Boolean): a tuple of the reward, state,\n",
        "            and boolean indicating if it's terminal.\n",
        "    \"\"\"\n",
        "\n",
        "    if action == 0: # UP (Task 1)\n",
        "        # Hint: Look at the code given for the other actions and think about the logic in them.\n",
        "        possible_next_loc = (self.agent_loc[0] - 1, self.agent_loc[1])\n",
        "        if possible_next_loc[0] >= 0 and possible_next_loc != self.obstacle: # Within Bounds?\n",
        "            self.agent_loc = possible_next_loc\n",
        "        else:\n",
        "            pass # Stay\n",
        "        ### END CODE HERE ###\n",
        "    elif action == 1: # LEFT\n",
        "        possible_next_loc = (self.agent_loc[0], self.agent_loc[1] - 1)\n",
        "        if possible_next_loc[1] >= 0 and possible_next_loc != self.obstacle: # Within Bounds?\n",
        "            self.agent_loc = possible_next_loc\n",
        "        else:\n",
        "            pass # Stay.\n",
        "    elif action == 2: # DOWN\n",
        "        possible_next_loc = (self.agent_loc[0] + 1, self.agent_loc[1])\n",
        "        if possible_next_loc[0] < self.grid_h and possible_next_loc != self.obstacle: # Within Bounds?\n",
        "            self.agent_loc = possible_next_loc\n",
        "        else:\n",
        "            pass # Stay.\n",
        "    elif action == 3: # RIGHT\n",
        "        possible_next_loc = (self.agent_loc[0], self.agent_loc[1] + 1)\n",
        "        if possible_next_loc[1] < self.grid_w and possible_next_loc != self.obstacle: # Within Bounds?\n",
        "            self.agent_loc = possible_next_loc\n",
        "        else:\n",
        "            pass # Stay.\n",
        "    else: \n",
        "        raise Exception(str(action) + \" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!\")\n",
        "\n",
        "    reward = -1\n",
        "    terminal = False\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Hint: Consider the initialization of reward and terminal variables above. Then, note the \n",
        "    # conditional statements and comments given below and carefully ensure to set the variables reward \n",
        "    # and terminal correctly for each case.\n",
        "    if self.agent_loc[0] == self.grid_h-1:\n",
        "      if 1 <= self.agent_loc[1] < self.grid_w-1:\n",
        "        reward = -100\n",
        "        self.agent_loc = (3,0)\n",
        "      elif self.agent_loc[1] == self.grid_w-1:\n",
        "        terminal = True\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n",
        "    return self.reward_state_term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "PivAmFqDUF-3",
        "nbgrader": {
          "checksum": "4a499bb0871653446d6b303c2625df09",
          "grade": false,
          "grade_id": "cell-88082a3dab94c4a8",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## env_cleanup()\n",
        "\n",
        "There is not much cleanup to do for the Cliff Walking environment. Here, we simply reset the agent location to be the start location in this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "O9pnhnylUF-3",
        "nbgrader": {
          "checksum": "ef29fa83791e5f064a53af742f2859a2",
          "grade": false,
          "grade_id": "cell-4c235eb2667f9f0d",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "%%add_to CliffWalkEnvironment\n",
        "\n",
        "# Do not modify this cell!\n",
        "\n",
        "# Work Required: No.\n",
        "def env_cleanup(self):\n",
        "    \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "    self.agent_loc = self.start_loc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OmCUa0LbUF-3",
        "nbgrader": {
          "checksum": "180b31b525fb94442eb79f07d7097700",
          "grade": false,
          "grade_id": "cell-ff4e65eb47d735c9",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "# Section 2. Agent\n",
        "\n",
        "In this second part of the assignment, you will be implementing the key updates for Temporal Difference Learning. There are two cases to consider depending on whether an action leads to a terminal state or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OxpITYwjUF-4",
        "nbgrader": {
          "checksum": "d4eaa5e48e54f9c5e55149325b215d47",
          "grade": false,
          "grade_id": "cell-3abcfd06cde56935",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell!\n",
        "\n",
        "# Create empty TDAgent class.\n",
        "# These methods will be filled in later cells.\n",
        "\n",
        "class TDAgent(BaseAgent):\n",
        "    def agent_init(self, agent_info={}):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def agent_start(self, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def agent_step(self, reward, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def agent_cleanup(self):        \n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def agent_message(self, message):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "WimVpxoxUF-4",
        "nbgrader": {
          "checksum": "83d69efb64983138188a2bad513622b1",
          "grade": false,
          "grade_id": "cell-edd826505e77e70a",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## agent_init()\n",
        "\n",
        "As we did with the environment, we first initialize the agent once when a TDAgent object is created. In this function, we create a random number generator, seeded with the seed provided in the agent_info dictionary to get reproducible results. We also set the policy, discount and step size based on the agent_info dictionary. Finally, with a convention that the policy is always specified as a mapping from states to actions and so is an array of size (# States, # Actions), we initialize a values array of shape (# States,) to zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "S4MMiKY7UF-4",
        "nbgrader": {
          "checksum": "24d451072e0b43f3ad7980887c6f5861",
          "grade": false,
          "grade_id": "cell-077135deef2f8fd9",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "%%add_to TDAgent\n",
        "\n",
        "# Do not modify this cell!\n",
        "\n",
        "# Work Required: No.\n",
        "def agent_init(self, agent_info={}):\n",
        "    \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
        "\n",
        "    # Create a random number generator with the provided seed to seed the agent for reproducibility.\n",
        "    self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
        "\n",
        "    # Policy will be given, recall that the goal is to accurately estimate its corresponding value function. \n",
        "    self.policy = agent_info.get(\"policy\")\n",
        "    # Discount factor (gamma) to use in the updates.\n",
        "    self.discount = agent_info.get(\"discount\")\n",
        "    # The learning rate or step size parameter (alpha) to use in updates.\n",
        "    self.step_size = agent_info.get(\"step_size\")\n",
        "\n",
        "    # Initialize an array of zeros that will hold the values.\n",
        "    # Recall that the policy can be represented as a (# States, # Actions) array. With the \n",
        "    # assumption that this is the case, we can use the first dimension of the policy to\n",
        "    # initialize the array for values.\n",
        "    self.values = np.zeros((self.policy.shape[0],))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "G-Z7Tv-oUF-4",
        "nbgrader": {
          "checksum": "a4f9d05505f09198739674b1b330b9cd",
          "grade": false,
          "grade_id": "cell-99e8e59cd1f7a5ef",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## agent_start()\n",
        "\n",
        "In agent_start(), we choose an action based on the initial state and policy we are evaluating. We also cache the state so that we can later update its value when we perform a Temporal Difference update. Finally, we return the action chosen so that the RL loop can continue and the environment can execute this action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "t4O_YrOMUF-5",
        "nbgrader": {
          "checksum": "8c5490c3d5fa88339b02cdf933f7f106",
          "grade": false,
          "grade_id": "cell-1b6dd05f7f49c1fc",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "%%add_to TDAgent\n",
        "\n",
        "# Do not modify this cell!\n",
        "\n",
        "# Work Required: No.\n",
        "def agent_start(self, state):\n",
        "    \"\"\"The first method called when the episode starts, called after\n",
        "    the environment starts.\n",
        "    Args:\n",
        "        state (Numpy array): the state from the environment's env_start function.\n",
        "    Returns:\n",
        "        The first action the agent takes.\n",
        "    \"\"\"\n",
        "    # The policy can be represented as a (# States, # Actions) array. So, we can use \n",
        "    # the second dimension here when choosing an action.\n",
        "    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
        "    self.last_state = state\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QMs-OtgdUF-5",
        "nbgrader": {
          "checksum": "fd9be2a7be0a8fbd3aee6b7c14f4ec37",
          "grade": false,
          "grade_id": "cell-a472a21b4a57b885",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## agent_step()\n",
        "\n",
        "In agent_step(), the agent must:\n",
        "\n",
        "- Perform an update to improve the value estimate of the previously visited state, and\n",
        "- Act based on the state provided by the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "60eJ-e_BUF-5",
        "nbgrader": {
          "checksum": "4c68b1daf60c5fbf3c75d4329048af15",
          "grade": false,
          "grade_id": "cell-2bec3235783127e8",
          "locked": false,
          "schema_version": 1,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "%%add_to TDAgent\n",
        "\n",
        "# Work Required: No. \n",
        "def agent_step(self, reward, state):\n",
        "    \"\"\"A step taken by the agent.\n",
        "    Args:\n",
        "        reward (float): the reward received for taking the last action taken\n",
        "        state (Numpy array): the state from the\n",
        "            environment's step after the last action, i.e., where the agent ended up after the\n",
        "            last action\n",
        "    Returns:\n",
        "        The action the agent is taking.\n",
        "    \"\"\"\n",
        "    # We should perform an update with the last state given that we now have the reward and\n",
        "    # next state. We break this into two steps. Recall for example that the Monte-Carlo update \n",
        "    # had the form: V[S_t] = V[S_t] + alpha * (target - V[S_t]), where the target was the return, G_t.\n",
        "    target = reward + self.discount * self.values[state]\n",
        "    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])\n",
        "\n",
        "    # Having updated the value for the last state, we now act based on the current \n",
        "    # state, and set the last state to be current one as we will next be making an \n",
        "    # update with it when agent_step is called next once the action we return from this function \n",
        "    # is executed in the environment.\n",
        "\n",
        "    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
        "    self.last_state = state\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0v713bxJUF-5",
        "nbgrader": {
          "checksum": "8f516e1bdb6ca3f1a1ce385cf2c56765",
          "grade": false,
          "grade_id": "cell-f5ed31e224e22cda",
          "locked": true,
          "schema_version": 1,
          "solution": false
        }
      },
      "source": [
        "## agent_end() \n",
        "\n",
        "TD update for the case where an action leads to a terminal state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "E6BEpgoTUF-5",
        "nbgrader": {
          "checksum": "65b123f954dd8176f089d389bc2e9735",
          "grade": false,
          "grade_id": "cell-08c5ac56c1a0a841",
          "locked": false,
          "schema_version": 1,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "%%add_to TDAgent\n",
        "\n",
        "# Work Required: No. \n",
        "\n",
        "def agent_end(self, reward):\n",
        "    \"\"\"Run when the agent terminates.\n",
        "    Args:\n",
        "        reward (float): the reward the agent received for entering the terminal state.\n",
        "    \"\"\"\n",
        "    # Here too, we should perform an update with the last state given that we now have the \n",
        "    # reward. Note that in this case, the action led to termination. Once more, we break this into \n",
        "    # two steps, computing the target and the update itself that uses the target and the \n",
        "    # current value estimate for the state whose value we are updating.\n",
        "    target = reward\n",
        "    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Escape_Room.ipynb",
      "provenance": []
    },
    "coursera": {
      "course_slug": "sample-based-learning-methods",
      "graded_item_id": "P4k5f",
      "launcher_item_id": "OwIbv"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
